# Chess.com End to End ETL Project

## **1.0 Project Overview**

This project aims to build an ETL pipeline to analyze my personal chess games and provide insights into the effectiveness of various chess openings. The data will be visualized on a live dashboard, enabling continuous tracking and analysis of performance over time. This article documents the preparation phase of the project.

### **1.1 Goals and Objectives**

- Analyze chess games(battles) across different time periods to determine:
    - Which openings(strategies & variations) have yielded the highest win rates.
    - Openings & Variations that frequently lead to losses.
    - Trends in performance over time based on opening.
- Generate Videos Of Good Games using PGN file and automate upload to Social Media
- Utilize StockFish API to analyze Game PGN FIles and Generate puzzles for missed wins and Focrced Checkmates
- Provide insights through a live dashboard.

---

## 2.0 Architecture & Workflow
![Pipeline Architecture](Project_Files/Chess_ETL_architecture_.gif)

### Data Flow
- Airflow pulls data from Chess.com API and stages it in the bronze layer of the data lake
- Duckdb Transforms and cleans data in bronze layer and then stores it in Silver layer
- Finally Duckdb Performs final aggregations and stores data in the dimensinoal model format in the gold layer. 
- Airflow loads data from the Gold layer into Data Warehouse.
- PowerBI is used to build dashboards and reports   
### ðŸ›  Tools & Technologies

| Category | Tools & Technologies Used |
| --- | --- |
| **Orchestration** | Apache Airflow |
| **Data Lake & Warehouse** | Azure Blob Storage, PostgreSQL |
| **Processing Engine** | DuckDB |
| **ETL Development** | Python, SQL |
| **Dashboarding** | Power BI |
| **Infrastructure** | Docker, Azure Services |



## 3.0 Data Sources & Storage

### **Data Sources:**

- **Chess API**: Fetches game records

### **Storage Structure:**

| Layer | Description |
| --- | --- |
| **Bronze** | Raw chess data (JSON) |
| **Silver** | Cleaned and transformed data (Parquet) |
| **Gold** | Data for fact and dimensional tables (PostgreSQL) |


## 4.0  Dimensional Data Model
![Pipeline Architecture](Project_Files/data_model.png)


## 5.0  **Approach & Pipeline Workflow (Airflow DAGS)**

### **Workflow Steps:**

1. **Extract**: Fetch chess game data from API and load unto Data Lake
2. **Transform**: Clean, structure, and 
3. **Load**: Store data into PostgreSQL.
4. **Visualization**: Power BI dashboard for insights

### DAG 1 
![Dag 2 Image](Project_Files/DAG_1.png)
This First Dag 
- Pulls Data from the CHESS.com website 
- Loads the Data into the bronze layer in the data lake in it's raw Json Format
- Once the data arrives, The data is transformed and cleaned using DuckDB using SQL Queries alongside User Defined Functions in the dags/scripts/python_scripts.py folder and then the data is loaded back into the Silver Layer of the Datalake
- The data is then finally converted to suit the data warehouse schema using the Fact table and dimensional table displayed earlier and then loaded into the Gold Layer of the datalake.


### DAG 2
![Dag 2 Image](Project_Files/DAG_2.png)
- This Final DAG is linked via the airflow dataset feature using the fact_table in the Gold layer. 
- Immediately the Fact table in the Gold layer is created, The DAG is created. 
- It First Created the Datawarehouse schema in the Postgres database if it does not already exists.
- Then it loads the data from the dim and fact tables in the gold layer into the postgres database.

## 6.0 Folder Directory
```bash  
â”œâ”€â”€ Data # Directory containing sample data used to test writing DAGS
â”‚Â Â  â”œâ”€â”€ 2024-01
â”‚Â Â  â””â”€â”€ opening
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Project_Files # Folder for storing images displayed in readme
â”‚Â Â  â”œâ”€â”€ Chess_ETL_architecture.mp4
â”‚Â Â  â”œâ”€â”€ Chess_ETL_architecture_.gif
â”‚Â Â  â”œâ”€â”€ DAG_1.png
â”‚Â Â  â”œâ”€â”€ DAG_2.png
â”‚Â Â  â””â”€â”€ data_model.png
â”œâ”€â”€ README.md
â”œâ”€â”€ __pycache__
â”‚Â Â  â””â”€â”€ utils.cpython-310.pyc
â”œâ”€â”€ airflow_settings.yaml
â”œâ”€â”€ config
â”œâ”€â”€ dags
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”œâ”€â”€ collect_chess_data_dag.py # Dag that pulls data from Chess.com API and stores in Data lake
â”‚Â Â  â”œâ”€â”€ load_data_warehouse_dag.py # Dag that loads data into the datwarehouse
â”‚Â Â  â”œâ”€â”€ notebooks # Notebooks used to create and test functions
â”‚Â Â  â”œâ”€â”€ scripts # Folder that holds all the functions used in each dag folder
â”‚Â Â  â””â”€â”€ sql # SQL queries used in DAG folder
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ include
â”€â”€ logs
â”‚Â Â  â”œâ”€â”€ dag_id=load_data_warehouse
â”‚Â Â  â”œâ”€â”€ dag_id=pull_data_from_chess_api
â”‚Â Â  â”œâ”€â”€ dag_processor_manager
â”‚Â Â  â”œâ”€â”€ dbt.log
â”‚Â Â  â””â”€â”€ scheduler
â”œâ”€â”€ openings.csv
â”œâ”€â”€ plugins
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ structure.txt
â”œâ”€â”€ tests
â”‚Â Â  â””â”€â”€ dags
â””â”€â”€ venv
    â”œâ”€â”€ bin
    â”œâ”€â”€ etc
    â”œâ”€â”€ include
    â”œâ”€â”€ lib
    â”œâ”€â”€ lib64 -> lib
    â”œâ”€â”€ pyvenv.cfg
    â””â”€â”€ share

28 directories, 16 files
